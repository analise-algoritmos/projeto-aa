\chapter{Notações e Conceitos Básicos}

Neste capítulo apresentamos os fundamentos matemáticos e conceituais necessários para a análise rigorosa da complexidade de algoritmos de ordenação. Abordamos as notações assintóticas fundamentais, técnicas de análise de recorrências, conceitos de estabilidade e otimalidade, bem como métodos de avaliação experimental. Este material fornece a base teórica essencial para compreender as análises detalhadas apresentadas nos capítulos subsequentes.

O domínio das notações assintóticas é crucial para expressar de forma precisa e concisa o comportamento de algoritmos em função do tamanho da entrada. Além das definições formais, apresentamos métodos práticos para determinar e provar limitantes de complexidade, técnicas para resolver relações de recorrência e estratégias para análise de casos médio, melhor e pior.

\section{Notações Assintóticas}

Denotamos por
$\mathbb{R}$ o conjunto dos números reais, 
$\mathbb{R}_{\geq 0}$ o conjunto dos números reais não negativos e
$\mathbb{R}_{>0}$ o conjunto dos números reais positivos.

\begin{defn} [Notação \textit{``Big O''}]
Dadas as funções $f:\mathbb{R}_{\geq 0}\rightarrow \mathbb{R}_{\geq 0}$ e $g:\mathbb{R}_{\geq 0}\rightarrow \mathbb{R}_{\geq 0}$, dizemos que $f(n)$ é $O(g(n))$ se existem constantes $c\in\mathbb{R}_{>0}$, $n_0\in \mathbb{R}_{\geq 0}$ tal que $f(n) \leq c\cdot g(n)$  para todo $n \geq n_0$. 
\end{defn}
\vspace{0.2cm}

Uma maneira equivalente de definir a notação \textit{``Big O''} é a seguinte:\vspace{0.2cm}

\begin{defn}
$O(g(n)) = \{ f(n):$ existem constantes $c\in\mathbb{R}_{>0}$, $n_0\in \mathbb{R}_{\geq 0}$ tal que $0 \leq f(n) \leq c\cdot g(n)$ para todo $n \geq n_0.\}$
\end{defn}
\vspace{0.3cm}

A notação ``Big O'' representa um \textbf{limite superior no tempo de execução} de um algoritmo.  Assim, ela fornece a \textbf{complexidade de tempo de pior caso} do algoritmo.

\textcolor{blue}{[... mantendo todos os exemplos e definições existentes ...].}

\section{Propriedades das Notações Assintóticas}

\begin{property}[Transitividade]
Se $f(n) = O(g(n))$ e $g(n) = O(h(n))$, então $f(n) = O(h(n))$.
\end{property}

\begin{property}[Reflexividade]
Para qualquer função $f(n)$, temos $f(n) = O(f(n))$.
\end{property}

\begin{property}[Simetria da notação $\Theta$]
$f(n) = \Theta(g(n))$ se e somente se $g(n) = \Theta(f(n))$.
\end{property}

\begin{property}[Regra da Soma]
Se $f_1(n) = O(g_1(n))$ e $f_2(n) = O(g_2(n))$, então $f_1(n) + f_2(n) = O(\max(g_1(n), g_2(n)))$.
\end{property}


\section{Análise de Recorrências}

\subsection{Método da Substituição}

O método da substituição consiste em três etapas:
\begin{enumerate}
\item \textbf{Hipótese:} Conjecturar a forma da solução
\item \textbf{Indução:} Usar indução matemática para verificar a hipótese
\item \textbf{Determinação de constantes:} Encontrar constantes que satisfaçam as condições
\end{enumerate}

\begin{exmp}
Considere a recorrência $T(n) = 2T(n/2) + n$ com $T(1) = 1$. Conjecturamos que $T(n) = O(n \log n)$.

\textbf{Prova:} Queremos mostrar que $T(n) \leq cn \log n$ para alguma constante $c > 0$.
\begin{align}
T(n) &= 2T(n/2) + n \\
&\leq 2c(n/2)\log(n/2) + n \\
&= cn(\log n - 1) + n \\
&= cn \log n - cn + n \\
&= cn \log n - (c-1)n
\end{align}
Para que $T(n) \leq cn \log n$, precisamos que $(c-1)n \geq 0$, ou seja, $c \geq 1$.
\end{exmp}

\subsection{Método da Árvore de Recursão}

\begin{exmp}
Para $T(n) = 3T(n/4) + \Theta(n^2)$:
\begin{itemize}
\item \textbf{Nível 0:} $n^2$
\item \textbf{Nível 1:} $3 \cdot (n/4)^2 = 3n^2/16$
\item \textbf{Nível 2:} $9 \cdot (n/16)^2 = 9n^2/256$
\item \textbf{Nível $i$:} $3^i \cdot (n/4^i)^2 = 3^i \cdot n^2/4^{2i} = n^2(3/16)^i$
\end{itemize}

A profundidade é $\log_4 n$ e o custo total é:
\begin{align}
T(n) &= n^2 \sum_{i=0}^{\log_4 n} (3/16)^i \\
&= n^2 \cdot \frac{1-(3/16)^{\log_4 n + 1}}{1-3/16} \\
&= \Theta(n^2)
\end{align}
\end{exmp}

\subsection{Teorema Mestre}

\begin{thm}[Teorema Mestre]
Seja $a \geq 1$ e $b > 1$ constantes, e seja $f(n)$ uma função. Se $T(n) = aT(n/b) + f(n)$, então:
\begin{enumerate}
\item Se $f(n) = O(n^{\log_b a - \epsilon})$ para alguma constante $\epsilon > 0$, então $T(n) = \Theta(n^{\log_b a})$
\item Se $f(n) = \Theta(n^{\log_b a})$, então $T(n) = \Theta(n^{\log_b a} \log n)$
\item Se $f(n) = \Omega(n^{\log_b a + \epsilon})$ para alguma constante $\epsilon > 0$ e se $af(n/b) \leq cf(n)$ para alguma constante $c < 1$ e $n$ suficientemente grande, então $T(n) = \Theta(f(n))$
\end{enumerate}
\end{thm}

\section{Conceitos Fundamentais para Algoritmos de Ordenação}

\subsection{Estabilidade}

\begin{defn}[Algoritmo Estável]
Um algoritmo de ordenação é \textbf{estável} se preserva a ordem relativa de elementos iguais. Formalmente, se elementos $x$ e $y$ são iguais e $x$ aparece antes de $y$ na sequência original, então $x$ deve aparecer antes de $y$ na sequência ordenada.
\end{defn}

\subsection{Ordenação In-Place}

\begin{defn}[Algoritmo In-Place]
Um algoritmo de ordenação é \textbf{in-place} se utiliza apenas uma quantidade constante de espaço auxiliar, isto é, $O(1)$ espaço extra além do array de entrada.
\end{defn}

\subsection{Algoritmos Adaptativos}

\begin{defn}[Algoritmo Adaptativo]
Um algoritmo de ordenação é \textbf{adaptativo} se sua performance melhora quando a entrada possui alguma estrutura pré-existente (como estar parcialmente ordenada).
\end{defn}

\subsection{Limite Inferior para Ordenação por Comparação}

\begin{thm}[Limite Inferior]
Qualquer algoritmo de ordenação baseado em comparações deve realizar pelo menos $\lceil \log_2(n!) \rceil$ comparações no pior caso para ordenar $n$ elementos.
\end{thm}

\textbf{Prova:} Considere a árvore de decisão do algoritmo. Cada folha corresponde a uma permutação possível dos elementos. Como há $n!$ permutações possíveis, a árvore deve ter pelo menos $n!$ folhas. Uma árvore binária com $n!$ folhas tem altura mínima $\lceil \log_2(n!) \rceil$.

Pela aproximação de Stirling: $\log_2(n!) = \Theta(n \log n)$.

\section{Medidas de Complexidade}

\subsection{Número de Comparações}

Para algoritmos baseados em comparação, frequentemente analisamos o número de comparações realizadas:
\begin{itemize}
\item \textbf{Limite teórico:} $\Omega(n \log n)$ comparações
\item \textbf{Algoritmos ótimos:} Merge Sort realiza $\Theta(n \log n)$ comparações
\item \textbf{Insertion Sort:} $O(n^2)$ comparações no pior caso, $\Theta(n)$ no melhor
\end{itemize}

\subsection{Número de Trocas}

\begin{defn}[Inversão]
Uma \textbf{inversão} em um array $A[1..n]$ é um par $(i,j)$ tal que $i < j$ mas $A[i] > A[j]$.
\end{defn}

\begin{property}
O número mínimo de trocas adjacentes necessárias para ordenar um array é igual ao número de inversões no array.
\end{property}

\subsection{Análise Amortizada}

\begin{defn}[Análise Amortizada]
A \textbf{análise amortizada} estuda o custo médio de uma sequência de operações, distribuindo o custo de operações caras sobre muitas operações baratas.
\end{defn}

\textbf{Métodos principais:}
\begin{enumerate}
\item \textbf{Método Agregado:} Analisa custo total de $n$ operações
\item \textbf{Método Contábil:} Atribui "créditos" a operações baratas
\item \textbf{Método Potencial:} Usa função potencial para redistribuir custos
\end{enumerate}

\section{Modelos de Custo e Complexidade de Cache}

\subsection{Modelo RAM}

No modelo RAM (Random Access Machine):
\begin{itemize}
\item Cada operação elementar tem custo constante
\item Acesso a qualquer posição da memória tem custo $O(1)$
\item Modelo adequado para análise assintótica básica
\end{itemize}

\subsection{Modelo Cache-Oblivious}

\begin{defn}[Complexidade de Cache]
Medimos o número de transferências entre cache e memória principal. Parâmetros:
\begin{itemize}
\item $M$: tamanho do cache
\item $B$: tamanho do bloco de transferência
\end{itemize}
\end{defn}

\textbf{Limitantes para ordenação cache-oblivious:}
\begin{itemize}
\item \textbf{Comparações:} $\Omega(n \log n)$
\item \textbf{Cache misses:} $\Omega\left(\frac{n}{B} \log_{M/B} \frac{n}{B}\right)$
\end{itemize}

\section{Técnicas de Análise Experimental}

\subsection{Metodologia}

\begin{enumerate}
\item \textbf{Implementação:} Código otimizado e correto
\item \textbf{Datasets:} Casos aleatórios, ordenados, reversos, com duplicatas
\item \textbf{Medições:} Tempo de CPU, comparações, trocas, cache misses
\item \textbf{Estatística:} Múltiplas execuções, intervalos de confiança
\end{enumerate}

\subsection{Análise de Constantes}

A notação assintótica oculta constantes importantes:
\begin{itemize}
\item \textbf{Quick Sort:} $\sim 1.39 n \log n$ comparações em média
\item \textbf{Merge Sort:} $\sim 1.00 n \log n$ comparações sempre
\item \textbf{Heap Sort:} $\sim 2.00 n \log n$ comparações no pior caso
\end{itemize}

\section{Resumo das Ferramentas de Análise}

\begin{table}[h]
\centering
\begin{tabular}{l|l|l}
% \hline
\textbf{Ferramenta} & \textbf{Aplicação} & \textbf{Exemplo} \\
\hline
Notação Big O & Limite superior & $T(n) = O(n^2)$ \\
% \hline
Notação $\Omega$ & Limite inferior & $T(n) = \Omega(n \log n)$ \\
% \hline
Notação $\Theta$ & Limite exato & $T(n) = \Theta(n \log n)$ \\
% \hline
Teorema Mestre & Recorrências divide-conquista & $T(n) = 2T(n/2) + O(n)$ \\
% \hline
Árvore de recursão & Recorrências complexas & Análise visual de custos \\
% \hline
Análise amortizada & Operações com custo variável & Arrays dinâmicos \\
% \hline
Potencial & Estruturas que mudam & Splay trees \\
\hline
\end{tabular}
\caption{Ferramentas de análise de complexidade e suas aplicações típicas}
\end{table}

\section{Exercícios}

\begin{exerc}
Prove que se $f(n) = O(g(n))$ e $g(n) = O(h(n))$, então $f(n) = O(h(n))$.
\end{exerc}

\begin{exerc}
Resolva a recorrência $T(n) = 4T(n/2) + n^2$ usando o Teorema Mestre.
\end{exerc}

\begin{exerc}
Prove que qualquer algoritmo que encontra o máximo de $n$ elementos deve realizar pelo menos $n-1$ comparações.
\end{exerc}

\begin{exerc}
Considere um algoritmo que mantém um array ordenado através de inserções. Se realizarmos $n$ inserções sequenciais, qual é o custo amortizado por inserção?
\end{exerc}

Este conjunto de conceitos e técnicas fornece a base necessária para a análise rigorosa dos algoritmos de ordenação apresentados nos próximos capítulos. Dominar estas ferramentas é essencial para compreender tanto os aspectos teóricos quanto práticos da eficiência algorítmica.


